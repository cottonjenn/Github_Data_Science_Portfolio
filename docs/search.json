[
  {
    "objectID": "projects/golf.html",
    "href": "projects/golf.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Spatial Determination of Number of Shots to Hole Out on a Professional Golf Hole"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Welcome to my data science portfolio!\nI’m a Statistics and Data Science major at BYU from Houston, Texas. This site shows my journey learning data science and analytics. Here you’ll find projects that demonstrate what I’ve learned and discovered.\nCheck out the About Me tab to know more about me and my background.\n\n\n\n\n\nIn this project, I’m developing a spatial model that predicts—with uncertainty—how many strokes a professional golfer needs to hole out from any location on the hole, using 1,764 real shots from 139 players on Hole 1 of the 2023 Players Championship at TPC Sawgrass. The raw shot coordinates came in an undocumented coordinate system, so I reverse-engineered the correct transformation using ArcGIS, built the full hole map from official shapefiles, and spatially joined every shot to precise boundaries (fairway, rough, bunkers, trees, green, fringe). I then fit both linear mixed-effects models (with player random intercepts) and an anisotropic 2D Matérn Gaussian process (Vecchia approximation in R’s gpgp package) to produce smooth, interpretable heatmaps of expected strokes remaining across the entire hole. This work demonstrates tough spatial data wrangling, coordinate system transformations, advanced mixed-effects and Gaussian process modeling, and turning complex geospatial data into actionable, visual insights. If you need someone who can clean, map, and model real-world location data end-to-end, this is a strong example.\n\n\n\nUsing machine learning techniques in Python, I developed predictive models to forecast post release player movement based on player tracking data from the NFL Big Data Bowl 2025 competition. Coming soon!\n\n\n\nI created a data-driven scouting report that pinpoints international prospects who specifically fix the Sacramento Kings’ biggest weaknesses after a 17-year playoff drought. Starting from mock data, I leveled it up by web-scraping real 2020-21 NBA stats from Basketball-Reference using Python, Selenium, and BeautifulSoup, defeating JavaScript lazy-loading that static methods couldn’t handle. I cleaned and merged the data, engineered advanced metrics, and built a weighted “Kings Fit Score” focused on rebounding, playmaking, defensive impact, and efficiency. The result: a ranked list of hidden-gem prospects who outperform the current roster exactly where Sacramento needs it most. This project showcases end-to-end web scraping, robust data cleaning, domain-driven algorithm design, clear visualization, and ethical data practices. If you’re looking for someone who can independently turn messy, real-world data into actionable insights—especially in sports analytics or high-impact decision support—this is a great example of how I work.\n\n\n\nTutorial Blog made in markdown explaining a simple SQL topic for Data Science\n\n\n\nComing soon! Project in my Data Science class (BYU Stat 386) coded in Python and markdown. I explored the relationship between MBA performance and salary scraped from baseball-reference.com. I made a python package to data clean, visualize, and analyze regression to find insights. I presented my findings in a report and Stream-lit app.\n\n\n\nLearn how I explore datasets to find interesting patterns and answer questions. Coming soon!\n\n\n\nI built this site using Quarto and host it on GitHub Pages.\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "This portfolio shows my work learning data science which includes:\n\nTutorial Blog made in markdown explaining a simple SQL topic for Data Science\nProjects in my Data Science class (BYU Stat 386) coded in Python and markdown\n\nI built this site using Quarto and host it on GitHub Pages."
  },
  {
    "objectID": "index.html#my-projects",
    "href": "index.html#my-projects",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "In this project, I’m developing a spatial model that predicts—with uncertainty—how many strokes a professional golfer needs to hole out from any location on the hole, using 1,764 real shots from 139 players on Hole 1 of the 2023 Players Championship at TPC Sawgrass. The raw shot coordinates came in an undocumented coordinate system, so I reverse-engineered the correct transformation using ArcGIS, built the full hole map from official shapefiles, and spatially joined every shot to precise boundaries (fairway, rough, bunkers, trees, green, fringe). I then fit both linear mixed-effects models (with player random intercepts) and an anisotropic 2D Matérn Gaussian process (Vecchia approximation in R’s gpgp package) to produce smooth, interpretable heatmaps of expected strokes remaining across the entire hole. This work demonstrates tough spatial data wrangling, coordinate system transformations, advanced mixed-effects and Gaussian process modeling, and turning complex geospatial data into actionable, visual insights. If you need someone who can clean, map, and model real-world location data end-to-end, this is a strong example.\n\n\n\nUsing machine learning techniques in Python, I developed predictive models to forecast post release player movement based on player tracking data from the NFL Big Data Bowl 2025 competition. Coming soon!\n\n\n\nI created a data-driven scouting report that pinpoints international prospects who specifically fix the Sacramento Kings’ biggest weaknesses after a 17-year playoff drought. Starting from mock data, I leveled it up by web-scraping real 2020-21 NBA stats from Basketball-Reference using Python, Selenium, and BeautifulSoup, defeating JavaScript lazy-loading that static methods couldn’t handle. I cleaned and merged the data, engineered advanced metrics, and built a weighted “Kings Fit Score” focused on rebounding, playmaking, defensive impact, and efficiency. The result: a ranked list of hidden-gem prospects who outperform the current roster exactly where Sacramento needs it most. This project showcases end-to-end web scraping, robust data cleaning, domain-driven algorithm design, clear visualization, and ethical data practices. If you’re looking for someone who can independently turn messy, real-world data into actionable insights—especially in sports analytics or high-impact decision support—this is a great example of how I work.\n\n\n\nTutorial Blog made in markdown explaining a simple SQL topic for Data Science\n\n\n\nComing soon! Project in my Data Science class (BYU Stat 386) coded in Python and markdown. I explored the relationship between MBA performance and salary scraped from baseball-reference.com. I made a python package to data clean, visualize, and analyze regression to find insights. I presented my findings in a report and Stream-lit app.\n\n\n\nLearn how I explore datasets to find interesting patterns and answer questions. Coming soon!\n\n\n\nI built this site using Quarto and host it on GitHub Pages.\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "projects/golf.html#abstract",
    "href": "projects/golf.html#abstract",
    "title": "",
    "section": "Abstract",
    "text": "Abstract\nGolf is a globally popular sport centered on precision and consistency. Players aim to complete a course in as few strokes as possible, navigating anywhere from 100 to over 600 yards from the tee box to the hole while avoiding strategically placed obstacles. This project investigates the spatial and strategic elements of professional golf, with an initial focus on both the number of strokes remaining to hole out, and the variability associated with this estimate. Using data from the first hole of the 2023 Players Championship at TPC Sawgrass—comprising 1,764 strokes taken by 139 unique golfers, including coordinate locations and lie characteristics—we have mapped and transformed this data into a structured format, incorporating shapefiles of the golf course and applying spatial transformations. Through exploratory analysis, we aim to understand spatial variation in stroke progression. Current efforts focus on refining and interpreting this analysis, laying the groundwork for future modeling of stroke progression. The ultimate goal is to estimate, with uncertainty, the predicted number of strokes remaining for any location on the hole, enabling more informed strategies for playing the course.",
    "crumbs": [
      "Spatial Determination of Number of Shots to Hole Out on a Professional Golf Hole"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "SQL Diagram\nHere is a graphic representation of SQL Join (focusing on INNER, LEFT, and (FULL OUTER types)). The inner join can be understood by the top left visual where 2 venn diagrams overlap (this is where the 2 tables are similar). Left join is keeping the left circle and where the 2 overlap (left table and where they are similar). Then the full (or outer) join is including everything in the diagram while also inserting “NA” or “None” where data is missing because of the join."
  },
  {
    "objectID": "blog.html#problem-statement",
    "href": "blog.html#problem-statement",
    "title": "Blog",
    "section": "Problem Statement",
    "text": "Problem Statement\nIn data science, combining data from multiple tables is common, but choosing the wrong SQL join can lead to incomplete or misleading results. This tutorial helps students understand how INNER, LEFT, and FULL OUTER joins affect outcomes when merging tables (e.g., customers and orders), and guides on selecting the right join for analysis tasks."
  },
  {
    "objectID": "blog.html#key-stepssections",
    "href": "blog.html#key-stepssections",
    "title": "Blog",
    "section": "Key Steps/Sections",
    "text": "Key Steps/Sections\n\n0. SQL Reminders\n\nSELECT: Specifies the columns you want to retrieve from your tables. For example, to get an animal’s name and species, you’d write SELECT name, species. In joins, you’ll often select columns from multiple tables, so clarity is key.\nFROM: Indicates the table(s) you’re querying. In a join, you’ll list multiple tables here, like FROM zoo_animals, exhibits, but we use JOIN to link them properly.\nON: Defines the condition for matching rows between tables in a join. For example, ON zoo_animals.animal_id = exhibits.animal_id links the tables using the animal_id column. This is the glue that makes joins work.\ntableinitial.columnname: When working with multiple tables, column names might overlap (e.g., both tables could have an id column). To avoid ambiguity, use the table’s alias or name followed by a dot, like zoo_animals.animal_id or exhibits.exhibit_name. Aliases (e.g., zoo_animals AS z) make this shorter: z.animal_id. This is critical for clear, error-free queries.\n; (semicolon): Always end your SQL statements with a semicolon. It tells the database your query is complete, like a period in a sentence. Forgetting it can cause errors in some SQL systems (e.g., PostgreSQL), though SQLite is more forgiving.\n\n\n\n1. Setup: Two Simple Tables\nLet’s start with two basic tables to illustrate joins. Imagine we have a customers table (with customer details) and an orders table (with purchase records). We’ll use a primary key (customer_id) in customers that acts as a foreign key in orders. For this static tutorial, we’ll define the tables directly as Markdown tables below. In a real SQL environment (like SQLite, MySQL, or PostgreSQL), you would create these tables using CREATE TABLE and INSERT statements.\nCustomers Table:\n\n\n\ncustomer_id\nname\nemail\n\n\n\n\n1\nAlice\nalice@email.com\n\n\n2\nBob\nbob@email.com\n\n\n3\nCharlie\ncharlie@email.com\n\n\n4\nDana\ndana@email.com\n\n\n5\nEve\neve@email.com\n\n\n\nOrders Table:\n\n\n\norder_id\ncustomer_id\nproduct\namount\n\n\n\n\n101\n1\nLaptop\n1200.0\n\n\n102\n2\nPhone\n800.0\n\n\n103\n1\nTablet\n300.0\n\n\n104\n6\nMonitor\n200.0\n\n\n105\n3\nKeyboard\n50.0\n\n\n\n\n\n2. INNER JOIN: Matching Only\nAn INNER JOIN returns only rows where there’s a match in both tables based on the join condition (e.g., customer_id).\nSELECT c.customer_id, c.name, o.order_id, o.product, o.amount\nFROM customers c\nINNER JOIN orders o ON c.customer_id = o.customer_id;\nExpected Result:\n\n\n\ncustomer_id\nname\norder_id\nproduct\namount\n\n\n\n\n1\nAlice\n101\nLaptop\n1200.0\n\n\n1\nAlice\n103\nTablet\n300.0\n\n\n2\nBob\n102\nPhone\n800.0\n\n\n3\nCharlie\n105\nKeyboard\n50.0\n\n\n\nNotice: Only matched customers (1,2,3) appear; Dana (4), Eve (5), and the unmatched order (104 for customer 6) are excluded.\n\n\n3. LEFT JOIN: Keep All from Left\nA LEFT JOIN keeps all rows from the left table (customers), with matching rows from the right (orders). Unmatched right-side columns get NULL.\nSELECT c.customer_id, c.name, o.order_id, o.product, o.amount\nFROM customers c\nLEFT JOIN orders o ON c.customer_id = o.customer_id;\n\n\n\ncustomer_id\nname\norder_id\nproduct\namount\n\n\n\n\n1\nAlice\n101\nLaptop\n1200.0\n\n\n1\nAlice\n103\nTablet\n300.0\n\n\n2\nBob\n102\nPhone\n800.0\n\n\n3\nCharlie\n105\nKeyboard\n50.0\n\n\n4\nDana\nNone\nNone\nNone\n\n\n5\nEve\nNone\nNone\nNone\n\n\n\nHere, all customers are included, with NULLS for Dana and Eve’s orders. Unmatched orders (like 104) are dropped.\n\n\n4. FULL OUTER JOIN: Everything, Everywhere\nA FULL OUTER JOIN (or just OUTER JOIN in some dialects) includes all rows from both tables, with NULLs where there’s no match. (Note: In SQLite, this is FULL OUTER JOIN; in MySQL, it’s emulated with UNION of LEFT and RIGHT joins.)\nSELECT c.customer_id, c.name, o.order_id, o.product, o.amount\nFROM customers c\nFULL OUTER JOIN orders o ON c.customer_id = o.customer_id;\n\n\n\ncustomer_id\nname\norder_id\nproduct\namount\n\n\n\n\n1\nAlice\n101\nLaptop\n1200.0\n\n\n1\nAlice\n103\nTablet\n300.0\n\n\n2\nBob\n102\nPhone\n800.0\n\n\n3\nCharlie\n105\nKeyboard\n50.0\n\n\n4\nDana\nNone\nNone\nNone\n\n\n5\nEve\nNone\nNone\nNone\n\n\n6\nNone\n104\nMonitor\n200.0\n\n\n\nThis shows everything: matched rows, plus unmatched customers (4,5) and the unmatched order (104).\n\n\n5. Comparing Join Results\nThis table depicts a summary about SQL Joins based off of the examples we used.\n\n\n\n\n\n\n\n\n\n\nJoin Type\nRows Returned\nIncludes Unmatched Left?\nIncludes Unmatched Right?\nUse Case Example\n\n\n\n\nINNER JOIN\n4\nNo\nNo\nGet only customers with orders\n\n\nLEFT JOIN\n6\nYes (with NULLs)\nNo\nAnalyze all customers, including those without orders\n\n\nFULL OUTER JOIN\n7\nYes (with NULLs)\nYes (with NULLs)\nAudit all data, spotting orphans in either table"
  },
  {
    "objectID": "blog.html#call-to-action",
    "href": "blog.html#call-to-action",
    "title": "Blog",
    "section": "Call to Action",
    "text": "Call to Action\n\nZoo Animals Table\nThis table lists animals in a zoo, with animal_id as the primary key.\n\n\n\nanimal_id\nname\nspecies\n\n\n\n\n1\nLeo\nLion\n\n\n2\nEllie\nElephant\n\n\n3\nGina\nGiraffe\n\n\n4\nPenny\nPenguin\n\n\n5\nToby\nTiger\n\n\n\n\n\nExhibits Table\nThis table lists exhibit assignments, with animal_id as a foreign key referencing the zoo_animals table.\n\n\n\nexhibit_id\nanimal_id\nexhibit_name\narea_sqft\n\n\n\n\n101\n1\nSavanna\n5000\n\n\n102\n3\nTall Plains\n3000\n\n\n103\n1\nBig Cats\n2000\n\n\n104\n6\nJungle\n4000\n\n\n105\n2\nElephant Enclosure\n6000\n\n\n\nNotes for Practice: - animal_id 6 in the exhibits table has no match in zoo_animals (unmatched right). - Animals like Penny (4) and Toby (5) in zoo_animals have no exhibits (unmatched left). - Use these tables to practice joins in an SQL editor like DB-Fiddle by creating them with CREATE TABLE and INSERT statements, similar to the SQL snippets in the previous tutorial."
  },
  {
    "objectID": "projects/data-acquisition.html",
    "href": "projects/data-acquisition.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#introduction",
    "href": "projects/data-acquisition.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nThis project originated as a mock assignment from a mentor simulating real-world NBA analytics work: the Sacramento Kings analytics staff needed recommendations for international players currently playing outside the NBA. The task provided mock JSON data on players who competed in both the NBA and major European leagues (EuroLeague, EuroCup, Spain’s ACB, and Italy’s Liga A), with the goal of demonstrating data ingestion, cleaning, exploratory data analysis, and communication skills.\nHowever, I saw an opportunity to expand this into a comprehensive data acquisition project for my portfolio. Rather than relying solely on the provided mock data, I decided to incorporate real NBA statistics by web scraping Basketball-Reference.com. This allowed me to ground the analysis in actual team performance data and demonstrate end-to-end data science skills—from scraping and cleaning to analysis and visualization.\nThe Sacramento Kings haven’t made the NBA playoffs since 2006—the longest active drought in professional sports. While the franchise has struggled with draft decisions and roster construction, one potential avenue for improvement lies in international scouting. International players offer tremendous value: they’re often overlooked, cost-effective, and can fill specific roster gaps that traditional college prospects might not address.\nBy combining real 2020-21 Kings performance data with mock international player statistics, this project demonstrates how data science can enhance basketball operations. Using web scraping, data cleaning, and exploratory analysis, I built a scouting report that identifies which international prospects best fit the Kings’ specific roster needs.",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#the-question",
    "href": "projects/data-acquisition.html#the-question",
    "title": "",
    "section": "The Question",
    "text": "The Question\nCan we use data to identify international prospects who specifically address the Sacramento Kings’ weaknesses?\nRather than simply finding the “best” players, I wanted to create a weighted fit score that matches prospects to the Kings’ actual gaps in rebounding, playmaking, shooting efficiency, and overall impact.",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#ethical-data-collection",
    "href": "projects/data-acquisition.html#ethical-data-collection",
    "title": "",
    "section": "Ethical Data Collection",
    "text": "Ethical Data Collection\nBefore diving into the technical work, I ensured my data collection followed best practices:\n\nBasketball-Reference.com’s robots.txt: I checked the site’s scraping policies and confirmed that accessing team statistics pages is permitted\nRespectful scraping: I implemented delays between requests and used Selenium only when necessary (not for bulk operations)\nMock data for prospects: My mentor provided anonymized international player statistics to simulate a real scouting database without privacy concerns\nPublic information only: All NBA data scraped is publicly available team-level statistics",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#the-web-scraping-challenge",
    "href": "projects/data-acquisition.html#the-web-scraping-challenge",
    "title": "",
    "section": "The Web Scraping Challenge",
    "text": "The Web Scraping Challenge\nGetting NBA data sounds straightforward—until you actually try it. Here’s what I learned.\n\nWhy Selenium Was Necessary\nI initially attempted to scrape the 2020-21 Sacramento Kings page using Python’s requests library and BeautifulSoup. This worked for basic tables like “Per Game Stats,” but Advanced Stats—the metrics I needed most—were completely missing.\nThe problem? JavaScript lazy-loading.\nBasketball-Reference doesn’t load all tables in the initial HTML response. Instead, JavaScript runs after the page renders in a browser, dynamically injecting critical tables like Advanced Stats, Shooting Stats, and Play-by-Play data. When you use requests.get(), you only receive the initial HTML—before JavaScript executes. The table wrappers exist (like &lt;div id=\"all_advanced\"&gt;), but they’re empty.\nThe solution? Selenium with Chrome WebDriver.\nBy launching a real browser, Selenium: - Executes JavaScript just like a human visitor would - Waits for DOM updates using WebDriverWait - Captures the fully rendered HTML via driver.page_source - Guarantees all 12 tables are present and parseable\n# Example Selenium code to scrape Basketball-Reference team page\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\n\n# Launch Chrome and load page\ndriver = webdriver.Chrome()\ndriver.get('https://www.basketball-reference.com/teams/SAC/2021.html')\n\n# Wait for Advanced Stats table to load\nWebDriverWait(driver, 10).until(\n    EC.presence_of_element_located((By.ID, \"all_advanced\"))\n)\n\n# Parse fully rendered HTML\nsoup = BeautifulSoup(driver.page_source, 'html.parser')\nTrade-off: Selenium is slower (~3-5 seconds per page) but achieves 100% accuracy. For this project, reliability mattered more than speed.",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#data-collection-steps",
    "href": "projects/data-acquisition.html#data-collection-steps",
    "title": "",
    "section": "Data Collection Steps",
    "text": "Data Collection Steps\nIf you want to replicate this project, here’s your roadmap:\n\nSet up your environment\n\nInstall Selenium: pip install selenium\nDownload ChromeDriver matching your Chrome version\nInstall BeautifulSoup: pip install beautifulsoup4\n\nScrape team data\n\nTarget URLs: Team pages (e.g., /teams/SAC/2021.html) and league overview (/leagues/NBA_2021.html)\nUse Selenium for JavaScript-heavy pages\nExtract tables using BeautifulSoup’s find_all('table')\n\nClean and merge data\n\nHandle missing values using median imputation by season\nCalculate per-game statistics from counting stats\nCompute advanced metrics like True Shooting % (TS%)\n\nPrepare your prospect database\n\nFilter for players under 25 years old\nRequire minimum 10 games played per season\nAggregate multi-season data into career averages",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#building-the-fit-score",
    "href": "projects/data-acquisition.html#building-the-fit-score",
    "title": "",
    "section": "Building the Fit Score",
    "text": "Building the Fit Score\nThis is where the project gets interesting. Instead of ranking players by raw talent, I created a Kings-specific fit score based on their 2020-21 weaknesses.\n\nStep 1: Identify Kings Weaknesses\nI compared the Kings to the league’s top 5 teams across 11 key metrics: - Scoring (points per game) - Playmaking (assists per game)\n- Rebounding (total rebounds per game) - Overall impact (Box Plus/Minus) - Efficiency (True Shooting %) - Ball security (turnovers—lower is better)\n\n\nStep 2: Assign Priority Weights\nNot all weaknesses matter equally. I assigned weights based on urgency: - Box Plus/Minus (20%): Overall impact is king - Rebounding (18%): The Kings were brutal on the boards - Assists (15%): Desperately needed playmaking - Turnovers (penalty): Lower turnovers = higher score\nWeights sum to 1.0 for transparency and tunability.\n\n\nStep 3: Calculate Fit Scores\nFor each international prospect:\nFit Score = Σ (Player's stat × Kings need weight)\nHigh scores indicate players who directly fill Sacramento’s biggest holes—not just “good” players, but the right players.",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#key-findings",
    "href": "projects/data-acquisition.html#key-findings",
    "title": "",
    "section": "Key Findings",
    "text": "Key Findings\n\nThe Sacramento Kings Were Below Average\nComparing the Kings to the league’s top 5 teams revealed significant gaps:\n\n\n\nTop 5 vs Kings Comparison\n\n\nThe bar chart below visualizes where Sacramento struggled most:\n Rebounding and free throw percentage were critical weaknesses\n\n\nTop 10 International Prospects\nAfter running my fit score algorithm, these prospects emerged as the best matches for the Kings:\n Ranked by fit score—how well they address Kings weaknesses\n\n\nProspect Skill Profiles\nThe heatmap reveals each prospect’s strengths:\n Darker colors indicate stronger performance in each category\n\n\nHead-to-Head Comparison\nHow do these prospects stack up against the 2020-21 Kings roster?\n Normalized comparison shows where prospects exceed Kings averages\nThe top prospects consistently outperformed Sacramento in rebounding, defensive impact (BPM), and shooting efficiency—exactly what the team needed.",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#technical-challenges-and-solutions",
    "href": "projects/data-acquisition.html#technical-challenges-and-solutions",
    "title": "",
    "section": "Technical Challenges and Solutions",
    "text": "Technical Challenges and Solutions\nChallenge #1: JavaScript-loaded content\nSolution: Switched from requests to Selenium\nChallenge #2: Inconsistent data formats across seasons\nSolution: Median imputation by season context\nChallenge #3: Avoiding division-by-zero errors in league comparisons\nSolution: Built fit scores using absolute values and weights instead of ratios\nChallenge #4: Handling missing Box Plus/Minus data\nSolution: Used player-level averages for Kings; neutral 0.0 for league baseline",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#conclusion",
    "href": "projects/data-acquisition.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nThis project demonstrates how data science can enhance basketball scouting beyond traditional “eye test” evaluations. By combining web scraping, statistical analysis, and domain knowledge, I identified international prospects who specifically address the Sacramento Kings’ weaknesses—something generic “best available player” rankings can’t do.\nThe methodology is transferable: change the team, adjust the weights, update the season, and you have a new scouting report. That’s the power of reproducible data science.",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#resources-and-links",
    "href": "projects/data-acquisition.html#resources-and-links",
    "title": "",
    "section": "Resources and Links",
    "text": "Resources and Links\n\nFull code repository: GitHub - Kings Scouting Report\nBasketball-Reference: Team Stats / League Stats\nSelenium Documentation: Getting Started\nBeautifulSoup Tutorial: Web Scraping Guide",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "projects/golf.html#introduction",
    "href": "projects/golf.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\n\nHole 1 Overview\nTo begin this project we used data from the 2023 PGA Tour Players Championship at TPC Sawgrass. We focused on the first hole, a par 4 with a length of 385 yards. The data included shot locations, strokes taken, and player information for 139 unique golfers.\n Hole 1 Overview: Image from pgatour.com\n\n\nCleaning data and Map\nIn our exploratory data analysis, we found some problems with the initial data. The X, Y, Z coordiantes were in an unknown coordinate system, so we had to transform them into latitude and longitude using ArcGIS and a linear model. Additionally, we had to make our own map of the hole using shapefiles from TPC Sawgrass and ArcGIS.\nHow we made our Hole 1 map in ArcGIS\nAfter transforming the coordinates and making the map, we joined the shot data with the map data to classify each shot based on its location on the hole (e.g., fairway, rough, bunker). We created a new variable called NewBoundary to represent these classifications and to easily define spatial variation than can occur on different sides of the whole despite similar terrain.\n\n\n\nplot_data",
    "crumbs": [
      "Spatial Determination of Number of Shots to Hole Out on a Professional Golf Hole"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#reflections-and-future-improvements",
    "href": "projects/data-acquisition.html#reflections-and-future-improvements",
    "title": "",
    "section": "Reflections and Future Improvements",
    "text": "Reflections and Future Improvements\nAfter completing this project, I identified a key methodological improvement for future iterations. Instead of comparing the Kings to league-wide averages, I should have scraped per-game and advanced statistics from each of the top 5 team rosters individually.\nThis roster-level approach would have allowed me to:\n\nPinpoint specific positional weaknesses by comparing the Kings’ roster construction to elite teams\nIdentify role-specific gaps rather than broad team averages\nCreate more granular fit scores that match prospects to exact roster holes\nAccount for lineup compositions and how individual players complement each other\n\nBy analyzing the actual rosters of championship-contending teams, the fit score algorithm would rely on where the Kings were truly weak relative to what makes teams successful—not just abstract league averages. This would produce more actionable scouting recommendations tied to proven winning formulas.",
    "crumbs": [
      "Finding Hidden Gems: A Data-Driven Scouting Report for the Sacramento Kings"
    ]
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nStatistics: Data Science - Brigham Young University, 2026\nRelevant Coursework: Statistical Modeling, Data Visualization, Data Wrangling, Bayesian Statistics, Predictive Analytics, Machine Learning, Data Science Ecosystems, Analysis of Correlated Data, Computer Science and Data Structures"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "Background",
    "text": "Background\n\nNFL Big Data Bowl 2026 Mentee\n\nMet with advisors from the NFL biweekly\nPrepared predictive analytic competition submission in R (current project so more information to come)\n\nUniversity of Utah Athletics Project:\n\nBuilt an R Shiny dashboard to visualize longitudinal strength data for coaches\nApplied mixed models and integrated athlete monitoring tools (VALD, CATAPULT)\nDelivered weekly presentations to stakeholders and mentors\n\nGolf Analytics Research at BYU:\n\nGeoreferenced maps and indivdual data points in ARCGIS\nEstimated strokes-to-hole from any course location using ArcGIS and advanced statistical modeling\nQuantified uncertainty in predictions and visualized spatial variability in R\n\nTeaching & Mentorship:\n\nDeveloped Python-based learning modules for engineering and science students\nTA for multiple statistics courses, supporting students in labs and lectures\n\nLanguages: Fluent in English and Spanish"
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "Skills & Interests",
    "text": "Skills & Interests\n\nTechnical Skills\n\nProgramming: Python, R, SQL, HTML, Linux\nData Analysis: Pandas, NumPy, DataExplorer, Dplyr, Tidyr, Corrplot\nVisualization: Matplotlib, Seaborn, Ggplot2, Leaflet, Plotly\nPredictive Analytics: Regression Trees, BART, Random Forest, Penalized Linear and Logistic Regression, Stacked Ensembles\nMachine Learning: Scikit-learn, Tidy-Models, Keras, TensorFlow\nTools: Jupyter Notebooks, Git/GitHub, Tableau, ArcGIS\n\n\n\nAreas of Interest\n\nSports (Knowledge of Game Rules and Regulations: Rugby, Volleyball, Football, Tennis, Golf)\nEnergy Systems and Consumption\nHumanitarian Aid\nCrime Analysis"
  },
  {
    "objectID": "about.html#get-to-know-me",
    "href": "about.html#get-to-know-me",
    "title": "About Me",
    "section": "Get to know me",
    "text": "Get to know me\n\nI love to play sports, paint, and do wedding makeup and hair.\nFrom Houston, Texas"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nEmail: worthenj.281@gmail.com\nGitHub: github.com/cottinjenn\nLinkedIn: linkedin.com/in/jenna-worthen-b9a8b2299\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "projects/golf.html#research-methodology",
    "href": "projects/golf.html#research-methodology",
    "title": "",
    "section": "Research Methodology",
    "text": "Research Methodology\nAfter cleaning and transforming the data, we focused on modeling the number of strokes remaining to hole out (StrokesRemaining) based on the shot location (NewBoundary) and round number (Round). We used a linear mixed-effects model to account for both fixed effects (NewBoundary and Round) and random effects (PlayerLastName) to capture individual player variability.\n\nExloring Spatial Variation through Linear Model\n\nLinear Mixed-Effects Model Notation\nLet:\n\n\\(y_{ij}\\) = StrokesRemaining for player \\(i\\) on observation \\(j\\)\n\\(\\beta_k\\) = fixed-effect coefficients for each NewBoundary category (because of -1, the model includes no intercept and estimates one coefficient per category)\n\\(\\gamma_r\\) = fixed-effect coefficients for each Round (factor) level\n\\(b_i \\sim N(0, \\sigma_b^2)\\) = random intercept for player \\(i\\)\n\\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\) = residual error\n\nThe model can be written as:\n\\[\ny_{ij} = \\sum_{k=1}^{K} \\beta_k \\cdot \\text{NewBoundary}*{ijk}\n;+\n\\sum*{r=2}^{R} \\gamma_r \\cdot \\mathbb{1}(\\text{Round}*{ij} = r)\n;+\nb_i\n;+\n\\epsilon*{ij}.\n\\]\nBy applying our model, we got the results below.\n\n\nLinear Mixed Model Output (Formatted Like lmer Summary)\nLinear mixed model fit by REML ['lmerMod']\nFormula: StrokesRemaining ~ -1 + NewBoundary + as.factor(Round) + (1 | PlayerLastName)\nData: filtered_data\n\nREML criterion at convergence: 1668.2\n\nScaled residuals:\n     Min      1Q  Median      3Q     Max\n -2.6382 -0.7887 -0.4565  0.8761  3.1484\n\nRandom effects:\n Groups          Name        Variance   Std.Dev.\n PlayerLastName  (Intercept) 0.0008314  0.02883\n Residual                     0.3509525 0.59241\nNumber of obs: 918, groups: PlayerLastName, 73\n\nFixed effects:\n                                       Estimate Std. Error t value\n NewBoundaryFairway                     2.65860    0.04928  53.944\n NewBoundaryFairway Bunker              3.22246    0.24284  13.270\n NewBoundaryFringe                      2.01567    0.26547   7.593\n NewBoundaryGreen                       1.43844    0.04346  33.096\n NewBoundaryGreenside Bunker Bottom     3.71937    0.18492  20.113\n NewBoundaryGreenside Bunker Top        3.17733    0.17165  18.511\n NewBoundaryRough                       3.13767    0.07157  43.839\n NewBoundaryTree Outline Left           3.47234    0.08905  38.992\n NewBoundaryTree Outline Right          3.40411    0.34537   9.856\n NewBoundaryTree Outline Top            3.62730    0.34712  10.450\n as.factor(Round)2                     -0.16437    0.05575  -2.948\n as.factor(Round)3                     -0.08638    0.05566  -1.552\n as.factor(Round)4                      0.03694    0.05537   0.667\n\nCorrelation matrix not shown by default, as p = 13 &gt; 12.\nUse print(x, correlation=TRUE) or vcov(x) if you need it.\nWith these results we learned that, for example, being in the fairway is associated with an estimated 2.66 strokes remaining to hole out, while being in the rough is associated with an estimated 3.14 strokes remaining, holding other factors constant. This indicates that players in the fairway are expected to require fewer strokes to complete the hole compared to those in the rough. Additionally, the random effect for PlayerLastName suggests that there is some variability in strokes remaining that can be attributed to individual player differences. As for the round effects, Round 2 shows a significant decrease in strokes remaining compared to Round 1, while Rounds 3 and 4 do not show significant differences.\n\n\nGPGP Spatial Model\nTo further explore spatial variation, we are working on implementing a model that models Strokes remaining using a mean-only (intercept) Gaussian process regression with an anisotropic 2D Matérn covariance function, implemented via the Vecchia approximation with 20 nearest neighbors using the R package gpgp.\n\nGPGP Model Notation\n\\[\ny_i = \\beta_0 + w(\\mathbf{s}_i) + \\varepsilon_i, \\quad i = 1,\\dots,n\n\\]\nwhere\n\n\\(y_i\\) = StrokesRemaining (observed strokes-to-go from location \\(i\\) on the green)\n\n\\(\\beta_0\\) = a global intercept (the only fixed effect — that’s why X is a column of 1s)\n\n\\(w(\\mathbf{s}_i)\\) = a zero-mean anisotropic 2D Matérn Gaussian process with\n\ncovariance function: matern_anisotropic2D\n\nsmoothness \\(\\nu = 1.5\\) (the default for Matérn in gpgp unless nu is changed)\n\ngeometric anisotropy (different range parameters in the two rotated directions)\n\n\n\\(\\varepsilon_i \\sim N(0, \\tau^2)\\) = independent nugget / measurement error\n\nThe GP is approximated with 20 nearest-neighbor basis functions (m_seq = 20), which is why you never need more than ~20 neighbors on a single golf green.\n\n\n\nIn standard spatial statistics notation\n\\[\ny(\\mathbf{s}) \\sim \\mathcal{GP}\\Bigl( \\beta_0,\\; \\sigma^2 \\cdot C_{\\text{aniso-Matérn}}(\\cdot;\\theta,\\psi) + \\tau^2 I \\Bigr)\n\\]\nwith Vecchia approximation using \\(m=20\\) neighbors and geometric anisotropy.\nThis approach allows us to capture spatial correlations and provide uncertainty estimates for our predictions. We will implement this approach to all boundary types for all rounds, in the hopes to create a smooth heatmap of predicted strokes remaining across the entire hole for every round. We used our player data to fit the model, then predicted across a grid of locations covering the hole to visualize spatial variation.\nPredicted Strokes Remaining based on the GPGP Model for our data vs smoothed across the entire grid",
    "crumbs": [
      "Spatial Determination of Number of Shots to Hole Out on a Professional Golf Hole"
    ]
  },
  {
    "objectID": "projects/golf.html#future-work",
    "href": "projects/golf.html#future-work",
    "title": "",
    "section": "Future Work",
    "text": "Future Work\nOur next steps involve refining the Gaussian Process model to better capture spatial dependencies and improve prediction accuracy. We plan to include other boundaries and rounds. Ultimately, we hope to develop a comprehensive tool that can assist golfers in strategizing their play based on spatial predictions of strokes remaining.\nStrokes Remaining for all Rounds using just our player data",
    "crumbs": [
      "Spatial Determination of Number of Shots to Hole Out on a Professional Golf Hole"
    ]
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: In this project, I’m developing a spatial model that predicts—with uncertainty—how many strokes a professional golfer needs to hole out from any location on the hole, using 1,764 real shots from 139 players on Hole 1 of the 2023 Players Championship at TPC Sawgrass. The raw shot coordinates came in an undocumented coordinate system, so I reverse-engineered the correct transformation using ArcGIS, built the full hole map from official shapefiles, and spatially joined every shot to precise boundaries (fairway, rough, bunkers, trees, green, fringe). I then fit both linear mixed-effects models (with player random intercepts) and an anisotropic 2D Matérn Gaussian process (Vecchia approximation in R’s gpgp package) to produce smooth, interpretable heatmaps of expected strokes remaining across the entire hole. This work demonstrates tough spatial data wrangling, coordinate system transformations, advanced mixed-effects and Gaussian process modeling, and turning complex geospatial data into actionable, visual insights. If you need someone who can clean, map, and model real-world location data end-to-end, this is a strong example.\n\n\n\nDescription: I created a data-driven scouting report that pinpoints international prospects who specifically fix the Sacramento Kings’ biggest weaknesses after a 17-year playoff drought. Starting from mock data, I leveled it up by web-scraping real 2020-21 NBA stats from Basketball-Reference using Python, Selenium, and BeautifulSoup, defeating JavaScript lazy-loading that static methods couldn’t handle. I cleaned and merged the data, engineered advanced metrics, and built a weighted “Kings Fit Score” focused on rebounding, playmaking, defensive impact, and efficiency. The result: a ranked list of hidden-gem prospects who outperform the current roster exactly where Sacramento needs it most. This project showcases end-to-end web scraping, robust data cleaning, domain-driven algorithm design, clear visualization, and ethical data practices. If you’re looking for someone who can independently turn messy, real-world data into actionable insights—especially in sports analytics or high-impact decision support—this is a great example of how I work.\n\n\n\nDescription: Using machine learning techniques in Python, I developed predictive models to forecast post release player movement based on player tracking data from the NFL Big Data Bowl 2025 competition. Coming soon!\n\n\n\nDescription: Coming soon! Project in my Data Science class (BYU Stat 386) coded in Python and markdown. I explored the relationship between MBA performance and salary scraped from baseball-reference.com. I made a python package to data clean, visualize, and analyze regression to find insights. I presented my findings in a report and Stream-lit app.\n\n\n\nDescription: Pick a dataset and explore it to discover insights and answer questions. Coming soon!",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#all-projects",
    "href": "projects/index.html#all-projects",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: In this project, I’m developing a spatial model that predicts—with uncertainty—how many strokes a professional golfer needs to hole out from any location on the hole, using 1,764 real shots from 139 players on Hole 1 of the 2023 Players Championship at TPC Sawgrass. The raw shot coordinates came in an undocumented coordinate system, so I reverse-engineered the correct transformation using ArcGIS, built the full hole map from official shapefiles, and spatially joined every shot to precise boundaries (fairway, rough, bunkers, trees, green, fringe). I then fit both linear mixed-effects models (with player random intercepts) and an anisotropic 2D Matérn Gaussian process (Vecchia approximation in R’s gpgp package) to produce smooth, interpretable heatmaps of expected strokes remaining across the entire hole. This work demonstrates tough spatial data wrangling, coordinate system transformations, advanced mixed-effects and Gaussian process modeling, and turning complex geospatial data into actionable, visual insights. If you need someone who can clean, map, and model real-world location data end-to-end, this is a strong example.\n\n\n\nDescription: I created a data-driven scouting report that pinpoints international prospects who specifically fix the Sacramento Kings’ biggest weaknesses after a 17-year playoff drought. Starting from mock data, I leveled it up by web-scraping real 2020-21 NBA stats from Basketball-Reference using Python, Selenium, and BeautifulSoup, defeating JavaScript lazy-loading that static methods couldn’t handle. I cleaned and merged the data, engineered advanced metrics, and built a weighted “Kings Fit Score” focused on rebounding, playmaking, defensive impact, and efficiency. The result: a ranked list of hidden-gem prospects who outperform the current roster exactly where Sacramento needs it most. This project showcases end-to-end web scraping, robust data cleaning, domain-driven algorithm design, clear visualization, and ethical data practices. If you’re looking for someone who can independently turn messy, real-world data into actionable insights—especially in sports analytics or high-impact decision support—this is a great example of how I work.\n\n\n\nDescription: Using machine learning techniques in Python, I developed predictive models to forecast post release player movement based on player tracking data from the NFL Big Data Bowl 2025 competition. Coming soon!\n\n\n\nDescription: Coming soon! Project in my Data Science class (BYU Stat 386) coded in Python and markdown. I explored the relationship between MBA performance and salary scraped from baseball-reference.com. I made a python package to data clean, visualize, and analyze regression to find insights. I presented my findings in a report and Stream-lit app.\n\n\n\nDescription: Pick a dataset and explore it to discover insights and answer questions. Coming soon!",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/nfl.html",
    "href": "projects/nfl.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nNFL Big Data Bowl 2025 Prediction Competition\nI have been working on this project since September 2025. Using machine learning techniques in Python, I developed predictive models to forecast post release player movement based on player tracking data from the NFL Big Data Bowl 2025 competition.\nI will update this page with more details about my methodology, data cleaning, and modeling approaches soon!",
    "crumbs": [
      "NFL Big Data Bowl 2025 Prediction Competition"
    ]
  }
]